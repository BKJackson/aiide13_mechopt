%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{todonotes}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{subcaption}

\frenchspacing
\setlength{\pdfpagewidth}{8.5in}
\setlength{\pdfpageheight}{11in}


\pdfinfo{
/Title (Formatting Instructions for Authors Using LaTeX)
/Subject (AAAI Publications)
/Author (anonymous)}
\setcounter{secnumdepth}{0}  



 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Interactive Bayesian Optimization for Game Mechanics}
\author{
}
% % AZ: note - technically we don't have the standard notion of ``interaction'' for the GP DDA



\maketitle
\begin{abstract}
\begin{quote}
\todo[inline]{fill me}
\end{quote}
\end{abstract}

\noindent Game design often involves a final phase of substantial fine-tuning of game mechanics. Paradigmatic examples include varying the settings of player character movement parameters, altering opponent combat statistics, or varying low-level parameters around movement and collision of game objects. Tuning is often a time-consuming and expensive process for several reasons:
\begin{enumerate}
\item parameter values must be set to (globally) optimal values, requiring search over a large space
\item evaluation of a set of parameter values cannot be done analytically or via simulation, but requires costly (in terms of time and money) direct human evaluation
\item quality of a set of parameters may be difficult to specify on a global scale, but instead be relative to other sets of parameters
\end{enumerate}
We ask the question: can an artificially intelligent system efficiently automate the tuning process? 
Can an artificially intelligent (AI) system act as a designer focused on the problem of parameter tuning?
Can such a system acquire design knowledge of low-level parameter settings that achieve design goals tailored to individual players? 

A game design AI system leverages elements from several existing paradigms for the use of AI in game design. Procedural content generation examines how an AI can create content within a space of parameters.  Dynamic difficulty adjustment concerns itself with adapting game mechanics in real time to changes in users. By contrast, we focus on the problem of learning a model of how game mechanics impact player behavior. Unlike player modeling, our goal is an AI system that actively adjusts game mechanics to explore the design space for that game both to achieve a given design objective and understand how that design space works. Rather than replace game designers, a low-level game design AI system allows for a new kind of design process where an AI system automatically player-tests games and intelligently searches for designs that meet a designer-specified goal. Building a model of the game design space allows the system to transfer learning from users across multiple design objectives.


We employ interactive Bayesian optimization in an AI system to choose parameter settings that are most informative about the design space while achieving given design goals for player behavior \cite{brochu2010:thesis}. By explicitly modeling the trade-off between exploring very different designs and exploiting designs similar to existing ones, our approach reduces the need for extensive amounts of play-testing while also automating the tuning process.
%Interactive Bayesian optimization (closely related to active learning \cite{settles2012:al-book} and sequential experimental design \cite{chaloner1995}) provides a method to optimize for design objectives that are expensive to evaluate \cite{brochu2010:thesis}. 
Employing non-parametric models of the game design space (here Gaussian Processes) we demonstrate the application of interactive Bayesian optimization to two cases studies of game design tuning in a shoot-'em-up game: (1) adjusting enemy design parameters to achieve a desired level of player success and (2) optimizing controls to player preferences.

% % Q: should we optimize controls or some other aspect?
For enemy design optimization we show how a designer-specified objective function for player performance statistics can guide building a regression model from enemy parameter settings to desired design features and reduce the need for large quantities of player testing. To optimize controls we use preference learning to select control settings to test and evaluate against the previous set of controls. 
In both studies, Bayesian optimization affords automatic exploration-exploitation trade-offs that enable rapidly (globally) optimizing for the design objective (player performance or preference).

% % Q: how evaluate preference model? compare against random? player-set? naive?

First, we discuss related work in game tailoring and adaptation. Second, we motivate and describe our interactive Bayesian optimization approach, detailing the Gaussian process regression and preference learning models. Third, we describe our shoot-'em-up game and describe two empirical human studies demonstrating the efficacy of our approach. We conclude by discussing extensions and the range of applications of this modeling approach.
% can elaborate on how model fits given some interpretive data

% % AZ: content overflow - process of exploring design space in conjunction w/user by letting user set parameters
% Finally, for joint player and enemy parameterization we demonstrate the use of a classification approach where players set player parameters and the system defines enemy parameters, seeking a desired design objective by actively setting enemies in ways to ensure that objective is met.


\section{Related Work}

% % % key ideas for us: learning over time rather than in batch mode; explore-exploit trade-off; generalization to many tuning techniques; conceptually about ``learning design space'' rather than just picking instances for a goal; minimize designer need to know system parameters while still providing flexibility to tune; probabilistic model means modeling (learning) and generation (sampling) come from unified souce
Automating low-level game design tuning relates to techniques for adapting content based on player behavior or preference information.
% % hand-crafted DDA models
Approaches to game tailoring and adaptation combine a player modeling technique with a content adaptation or generation method. Many early efforts employed a game-specific player model using vector of attributes (e.g. skills or use of content) and reactively selected new content to guide players toward a desired level of skill or intended level of performance. Hunicke and Chapman \shortcite{hunicke2004:dda} track the average and variance of player damage and inventory levels and employ a hand-crafted policy to adjust levels of enemies or powerups. \cite{magerko2006:isat}, \cite{el-nasr2007}, and \cite{thue2007:storytell-pm} model players as vectors of skills, personality traits, or pre-defined ``player types'' and select content to fit players using rule-based approaches. 

In contrast, a Bayesian optimization approach generalizes across games, automatically determines how complex the player model should be, and does not require designers to tune a set of rules (or other parameters) to generate content while retaining designer control over the algorithm objectives. 
We seek an approach that is intentionally attempting to acquire design knowledge relevant to designer-specified goals, without requiring a designer have deep familiarity with either the player modeling or generation algorithms.

% % simulation approaches
% % AZ: unneeded as we're interested in adaptation, not generation w/o
%\cite{sorenson2011} \cite{cook2013:mechanicminer} \cite{togelius2008:gamegen} \cite{hom2007:gamegen} \cite{browne2010:ludi}
%sorenson + pasquier - requires simulation model; we don't want designers to need model of full domain / overall game quality elements, but use humans to guide process

% % encoding designer goals to get results
% % AZ: unneeded as we're interested in adaptation, not just design implementation
%\cite{treanor2012:gameomatic} \cite{smith2010:ludocore} \cite{smith2010:variations}
%methods are good for exploring design space but ignore actual way players play


% % ML/EC based models that have no principled exploration-exploitation trade-off --> our system is trying to ``learn'' space of options
Evolutionary computing and machine learning provide frameworks for modeling players and optimizing game parameters to achieve adaptation goals.
\cite{hastings2009:gar} tracks player use of weapons and uses neuro-evolution to generate new weapons based on those expressed preferences.
\cite{pedersen2009:smb} employs neuro-evolution to model reported frustration, challenge, and fun with a multi-layer perceptron. 
\cite{shaker2010:platformer-gen} use this model to generate platformer game levels through an exhaustive search process. 
\cite{yannakakis2009:playermodel} employs the same techniques in an augmented reality game setting, attempting to optimize player satisfaction.
\cite{missura2009:dda} create player clusters from training data and train a linear regression models for game difficulty within each cluster in an arcade game. During adaptation, new player clusters are predicted from early play data using a support vector machine (SVM) and difficulty is set using the cluster-specific regression model.
\cite{yu2011:minboredom} performs a similar process of player modeling and adaptation by clustering players and optimizing game parameters employing a large-margin SVM approach that minimizes player boredom or frustration in a level-based action games.

Unlike these approaches we integrate feature selection into the model selection process, automatically balance model complexity with fit (rather than requiring parameter tuning), and automatically trade off exploration of possible parameter settings against exploitation (rather than only exploiting through optimization). Conceptually, the interactive Bayesian optimization approach captures a full design tuning process of testing many possible parameter settings, incrementally updating an understanding of the design space, and selectively seeking points in the space that will be most informative to design goals while still meeting design objectives.


%\cite{yu2012:prefix-based} CF w/fixed alternatives, hard to scale to continuous parameter spaces
%\cite{zook2012:tf} large-scale data, not iterative 
%\cite{pons2012:scenadapt} no explore-exploit; pref-based model unclear

%\cite{yannakakis2009:gameadapt} \cite{yannakakis2009:playermodel} GP pref model employed, not found best. however, we use more sophisticated kernel fn in order to better adjust fit parameters across dimensions and get back information on relative importance of dimensions. also note that GP pref generally has highest performance except in one case


% % adaptation frameworks
%\cite{bakkes2012} - not really applicable, but talking about personalization in terms of AI opponents at level of tactics and strategy

%\cite{togelius2011:sbpcg} \cite{yannakakis2011:edpcg} are overall frameworks of things to do, but give little attention to needs of explore-exploit modeling, overfit, and other ML concerns that are crucial for succeeding; little attention to how system improves over time, rather than just attempts to fit

% % AZ [NOTE] - I was explicitly terse about related work, but can easily expand out to cover other aspects as needed. As it stands the most important things are covered, but I'd forgotten about 1-page allowed for references

% % AZ: brief introduction to the ideas
\section{Interactive Bayesian Optimization}
Sequential Bayesian optimization optimizes a function through iteratively testing a sequence of points, each one selected by some algorithm based on previous points. Two functions are involved: (1) the \textit{objective function} that maps inputs to outputs; and (2) the \textit{acquisition function} that maps potential inputs to their value for optimizing the objective function output. In our application we employ Gaussian processes (GPs) for the objective function---the game design model from parameters to player behavior---and a modified expected improvement acquisition function---the designer objective for the game design. Using player feedback makes the process a type of interactive Bayesian optimization \cite{brochu2010:thesis}.

Gaussian processes are a widely used non-parametric modeling technique able to capture complex non-linear relationships within a data set, automatically adjust model complexity to data, and integrate out parameters without user intervention \cite{rasmussen2006}. Intuitively, non-parametric models are models that allow for an infinite number of variables to account for the data before selecting only the subset needed to explain a given set of observations. In practice, this leads to models that automatically become more complex to fit a data set as needed. Bayesian formulations of GP regression and classification automatically trade-off between complexity of a model and fit to a data set, avoiding over-fitting and poor generalization problems that occur with optimization approaches. Bayesian model specifications allow parameters of the model to be integrated out, simplifying their use by requiring less user specification. We employ GPs to leverage the benefits of non-linear mapping from inputs to outputs, automatic complexity adjustment with data collection, and reduced or eliminated parameter specification from users.

Below we describe the formulation of GP regression and GP preference learning and then integrate GP models with active learning methods. Gaussian process regression enables automatic difficulty adjustment by modeling player performance in a game as a non-linear function of game parameters. Gaussian process preference learning enables optimization of game parameters (here controls) to player preferences by modeling player preferences for a set of game parameters as a non-linear function of game parameters than is then forced to pairwise choices between alternatives. Active learning uses a GP objective function to identify desirable next parameter settings to test, guided by a designer-specified acquisition function---here expected improvement---for parameter adjustment. For game performance, designers specify a goal of achieving a given level of in-game performance. For controls, designers specify optimal player preference.

\subsection{Gaussian Process Regression}
Gaussian processes are formally defined as ``a collection of random variables, any finite number of which have a joint Gaussian distribution'' \cite{rasmussen2006}. While allowing an infinite number of variables to be used, any GP model can be computed through a multivariate Gaussian distribution based on the input and output values. Gaussian processes are specified by their mean function ($m(x)$) and covariance function ($k(x,x')$):

$$ f(x) \thicksim GP( m(x), k(x,x') ) $$
Intuitively, GP regression learns a model predicting that similar inputs---according to the covariance function---should yield similar outputs. Different choices of the covariance function define different notions of similarity. In our work we employ the automatic relevancy detection (ARD) version of a squared exponential distance:

$$ k(x,x') = exp\big( -\frac{1}{2} \Sigma_{l=1}^{d} \kappa^{l} (x^{l} - x'^{l})^2 \big) $$
where $\kappa^{l} > 0$ is the ARD parameter for the $l$-th feature of a {d}-dimensional data set, serving to control the contribution of this feature to the model. Automatic relevancy detection allows us to optimize model parameters during the fitting process, automatically scaling input dimensions to minimize the impact of irrelevant aspects of the data. Mathematical properties of the GP mean that an initially zero valued mean function will taken on non-zero values after fitting data, allowing the model to be initialized with zero as the mean value (see \cite{rasmussen2006} for additional details on GP regression). In our case we will use such a zero-mean GP.

For our performance regression model we predict player performance (number of times hit) from game parameters controlling enemy attacks (speed and size of bullets along with firing rate). We fit a GP regression model to player data and optimize the covariance function ARD parameters using stochastic gradient descent after each training point received. Since GP regression has a closed-form solution for learning and prediction this task can be done in near-real time with no appreciable time requirements ($< 1$ second for \todo[inline]{N training points}).

\subsection{Gaussian Process Preference Learning}
We employ a pairwise preference learning model rather than using preference rating scales due to human biases. Sequential numeric ratings are subject to a cognitive anchoring bias where earlier numeric ratings influence choices on subsequent ratings \cite{tversky1974:biases}. We thus employ a model that generalizes information gained from pairwise rankings to the underlying preference of users for different instances (here game parameter settings). Games can only be played sequentially during comparisons, motivating an approach of pairwise preference ratings comparing each new instance to the previous one.

Gaussian process preference learning models user choices in a two-part model: (1) a GP regression model specifying the underlying  (unobserved) value of a single instance; (2) a probit model of how a choice is generated based on two instance being compared \cite{chu2005}. The GP model allows a flexible specification of how users value a given instance specified in terms of its parameters. The probit model---known in economics as the Thurstone-Mosteller law of comparative judgment---converts a pair of latent values into a comparison judgment according to the function:

$$ P( x_i \succ x_j | f(x_i), f(x_j) = \Phi\bigg( \frac{f(x_j) - f_(x_j)}{\sqrt{2} \sigma_{noise}} \bigg) $$
where $x_i, x_j$ are two instances, $f(x_i)$ is the GP latent value of an instance, $\Phi$ is the cumulative normal distribution, and $\sigma_{noise}$ is the inherent noisiness of comparative judgments. Intuitively, the probit model encodes preference judgments as based on the difference in underlying value of two instances, allowing for noise in preference ratings. 

Due to the non-linear probit model used GP preference learning has no analytic learning model. Instead, we follow work by \cite{chu2005} and use a Laplace approximation to learn the underlying GP model's parameters. We employ a GP with zero mean and the ARD covariance function and optimize its parameters along with the selection of $\sigma_{noise}$ using a grid search over the space of possible parameters \todo{better optimization method}. Unlike GP regression fitting, these nested optimization processes are computationally expensive, requiring \todo{X seconds to fit N training points with D dimension}. However, we note that optimization may be performed using any-time algorithms (such as DIRECT \cite{jones1993:direct}), allowing optimization of parameters to occur while the player plays a new option. In our experiments we chose to optimize parameter values and force players to wait in order to test the best-case performance of our approach.


\subsection{Active Learning}
Active learning (AL) is an approach to machine learning problems with a large set of unlabeled instances where a computer asks a human to provide information about given instances to learn a model of the instances as a whole \cite{settles2012:al-book}. AL is well suited to our application where the space of game parameterizations is very large and information can only be gained through the expensive process of having a human play and provide feedback about a game instance. Acquisition functions specify how a given AL algorithm weights potential instances to test based on a goal of optimizing the objective function. In our case, the GP regression model seeks to minimize the difference between desired and actual player performance and the GP preference model seeks to find the highest latent value instance.

Many possible acquisition functions exist, varying in how the functions balance the exploration-exploitation trade-off guiding how locally-tethered the search for large objective functions is \cite{settles2012:al-book}. Expected improvement (EI) is an acquisition function that balances the value of unseen instances against the uncertainty regarding their values. EI integrates over all possible results to get an average-case estimate of the result, rather than seeking a best-case (or worst-case) scenario. We employ a modified EI function that incorporates a slack parameter ($\xi \ge 0$) to control the relative weighting of exploration and exploitation goals \cite{lizotte2008}:


% % clean up formatting to fit width
$$ EI(x) = \begin{cases} 
(f(x) - f(x^{+}) - \xi \Phi(Z) + \\ \sigma(x) \phi(Z)) & \mbox{if } \sigma(x) > 0 \\ 
0 & \mbox{if } \sigma(x) = 0 
\end{cases} $$
where $f(x)$ is the function value at $x$, $x^{+}$ is the instance with the current greatest function value, $\sigma{x}$ is the uncertainty in the value of the instance, $\phi(Z)$ is the Gaussian distribution density at $Z$ and $Z$ is defined as:

$$ Z = \begin{cases} 
\frac{f(x) - f(x^{+}) - \xi}{\sigma(x)} & \mbox{if } \sigma(x) > 0 \\ 
0 & \mbox{if } \sigma(x) = 0 
\end{cases} $$
Intuitively, $Z$ is the noise-scaled difference between the test point $x$ and the current best point $x^{+}$, and the expected improvement takes a weighted combination of this gain against the uncertainty of the point. Points that are more uncertain and expected to have higher values are preferred to those with lower values or high values that are highly certain. $\xi$ allows an explicit specification of how heavily to emphasize exploration



\section{Experiment}
% % goal = assess model ability to:
% % % regression = fit player to target behavior
% % % preference = get controls that "suit" player --> Q: how know when "good enough"?
We test our mechanics adaptation approach in a shoot-'em-up arcade game getting (Figure \ref{fig:shmup}). An arcade-style game enables our system to use a series of waves to test game parameter settings and gather feedback from the player, iteratively refining the parameter settings to a given design objective. The fast, reflex-based gameplay style of the game helps reduce complexities involved in long-term player planning and strategy.

In this section we describe our game domain and the two empirical human studies we conducted. The first study examines the use of GP regression to find and continually adjust game parameter settings to achieve a designer-specified level of performance. We show the method can adapt parameters to achieve a desired level of performance while learning a player model that fits the data with fewer data points needed.
The second study examines the use of GP preference modeling to optimize controls to player preferences. We show our approach reduces the amount of player data needed compared to random sampling \todo{and? greater ratio of better/worse?}.

\subsection{Game Domain}

\begin{figure}[tbph]
\centering
\includegraphics[width=1\linewidth]{./bullethell_sidebyside}
\caption{Experimental game interface illustrating player, enemies, and shots fired by both at two points along adaptation process. Preference learning experiments varied parameters related to player movement; enemy tuning involved the speed, size, and rate of fire of enemy bullets.}
\label{fig:shmup}
\end{figure}



% % SHMUP description: score, params
Shoot-'em-up games emphasize reflexes and pattern recognition abilities as a player maneuvers a ship to dodge enemy shots and return fire. In our game players are scored based on the number of times they are hit: players continually gain points over time and lose points when hit by opponents\todo{update scoring as needed}. Players are encouraged to avoid enemies as best as possible either through dodging enemy attacks or destroying enemy ships to reduce the number of incoming attacks.

% % control params
Player controls are limited to shooting at enemies and moving their ship. Ship movement is governed by two controllable parameters: drag and thrust. Drag is the ``friction'' applied to a ship that decelerates the moving ship at a constant rate when it is moving---larger values cause the ship to stop drifting in motion sooner. Thrust is the ``force'' a player movement press applies to accelerate the ship---larger values cause the ship to move more rapidly when the player presses a key to move. Combinations of thrust a drag are easy to tune to rough ranges of playability. However, the precise values needed to ensure the player has the appropriate level of control are difficult to achieve as player movement depends on how enemies attack and individual player preferences for control sensitivity (much like mouse movement sensitivity). Our preference-learning experiment allowed the system to set drag and thrust for the player while attempting to maximize player preference for a set of controls.
% % note: should we be tuning something else that is more "complicated"?

% % enemy params
During each wave of enemies a series of opponents are created that fire bullets at the player. Enemy parameters used in our study consist of the: size of enemy bullets, speed of enemy bullets, and rate that enemies fire bullets. Increasing bullet size requires the player to move more carefully to avoid bullets. Faster bullets require quicker player reflexes to dodge incoming fire. More rapid firing rates increase the volume of incoming fire. Together these three parameters govern how much players must move to dodge enemy attacks, in turn challenging player reflexes. As with controls, getting approximate settings for these parameters is easy, but fine-tuning them for a desired level of difficulty can be challenging. Our performance adaptation experiment allowed the system to set enemy bullet size, speed, and firing rate to minimize the difference between player performance and designer-specified target performance.


\subsection{Methods}
% % conditions test different cases for regression-based adaptation + preference learning
We conducted two studies of our system using 30\todo{update} players in both studies. In the studies players logged into the game online and played the game against a series of waves of enemies. Players were randomly assigned to a control case that sampled random possible game parameter values and a test case that selected parameter values using interactive Bayesian optimization. 
%Players were allowed to play up to 5\todo{update}, 30 second-long waves of enemies in a fixed tutorial mode to familiarize themselves with the game. For both studies players completed 15\todo{update} waves of enemies. 
We recorded player performance in terms of number of times the player was hit and all parameter settings for each wave.
For analysis we examined only results from players who completed at least ten waves and only examine data from those ten waves.

In the performance optimization study a GP regression model was fit to predict the difference between the actual number of times the player was hit and a desired designer-specified value. We specified a designer target of players being hit 6 times over the course of a 20 second wave of combat. Between waves the model and covariance function parameters were all optimized and EI used to select the next test point.
Learning had a one wave lag---players were allowed to play a wave while the model was fit to all data but the most recent wave. This enabled the game to play continuously without pausing for learning to occur at the cost of a model that was unaware of the most recent test point results.

In the control optimization study a GP preference model was fit to predict the underlying preference players had for different control settings. After each wave players were prompted to indicate whether the most recent wave had better or worse controls than the wave before that. Model fitting used the same timing as above. In this case EI selected an instance to test in comparison to the last tested instance---every comparison was the next point that would be most useful compared to the last level the player completed.



\subsection{Results}



\section{Discussion}
% % value for optimizing subjective or objective results

% % note: can combine multiple aspects into single acquisition function, but then need additional complexity of learning how to balance objectives if not told directly; e.g. preference + performance

% % enable continuous model improvement

% % future: optimal experimental design (rafferty, chaloner); hcomp for more direct human participation


\section{Acknowledgments}
% Eric Fruchter - or possible co-author

\bibliographystyle{aaai}
\bibliography{lib}

\end{document}
